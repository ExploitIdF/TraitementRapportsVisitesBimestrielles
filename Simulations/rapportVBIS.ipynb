{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation des rapports des visites bimestrielles et des interventions de maintenance annuelle des issues et des niches \n",
    "\n",
    "* Définition du calendier et des numéros d'OT (fermeturesJours.csv)\n",
    "* Production des rapports (absences aléatoires)\n",
    "* Upload des rapports sur le serveur\n",
    "\n",
    "Cette simulation permet de tester les choix qui sont faits sur la forme des rapports.  \n",
    "la simulation fournit un jeu de données pour développer les outils de traitements des données pour la visualisation des résultats.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -r requirements.txt\n",
    "import pandas as pd, numpy as np\n",
    "import glob, re,json,io,os\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "import plotly.graph_objects as go\n",
    "pd.options.display.max_colwidth = 100\n",
    "from numpy.random import random\n",
    "from numpy.random import choice\n",
    "rng = np.random.default_rng()\n",
    "from google.cloud import bigquery,storage\n",
    "project_id = 'tunnels-dirif'\n",
    "clientB = bigquery.Client(project_id)\n",
    "storageC=storage.Client(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmation des dates de fermetures et simulation des \n",
    "Pour chaque fermeture, on définit aléatoirement 6 dates de visites bimestrielles (champ \"jour\").  \n",
    "Les dates sont identifiées par le rang du jour dans l'année (dayofyear).   \n",
    "On définit les OTs père (champ \"OTP\" : 456000+k*100+i  ). Les OTPs sont dans la plage 456000+456600  \n",
    "On simule que des visites bimestrielles ne sont pas faites ou sont faites, mais reportée.  \n",
    "Pour cela, on définit le champ 'faitFerm' avec mes valeurs (OK : fait à la date programmée, RP: fait mais reporté 21 jours plus tard & KO: non fait).    \n",
    "Paramètres du calcul : ```tauxRéalisation & tauxReport```   \n",
    "\n",
    "On crée la table des visites d'issues programmées `VisiteIssues`\n",
    "On associe aux issues les dates de leurs fermetures programmées .    \n",
    "On définit les OT fils (OTFs) des visites d'issues : ordreVB*300+indexIss+456600  \n",
    "On simule que lors d'une visites bimestrielles certaines issues ne donnent pas lieu à la transmission d'un rapport.   \n",
    "Pour cela, on définit le champ 'faitIss' avec mes valeurs (True / False).  \n",
    "Paramètres du calcul : `tauxAbsenceIss`\n",
    "\n",
    "On crée la table des rapport de visites d'issues transmis (faits) `VisiteIssuesFt` en sélectionnant avec les champs ```faitFerm & faitIss```    \n",
    "On définit l'horodate de transmission du rapport par une valeur aléatoire comprise entre jour + 23h et jour + 27h (jour+1 +3h)   \n",
    "Pour les visites reportées (``` faitIss = 'RP' ```) on calcule une nouvelle horodate 21 jours plus tard (on aurait pu faire varier le report mais on a simplifié)   \n",
    "Enfin, on choisit aléatoirement un nom d'agent (il aurait été plus logique de choisir les agents au niveau de la fermeture mais c'est plus simple comme ça).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "issues=pd.read_csv('https://raw.githubusercontent.com/ExploitIdF/Referentiel_Tunnels/refs/heads/main/issuesFermetures.csv')#[[  'Tatouage',   'Fermeture']]\n",
    "# ['CodeEx', 'PC', 'Tatouage', 'triCode', 'Fermeture']\n",
    "issues['OQ']=range(290)\n",
    "fermetures=issues['Fermeture'].value_counts()\n",
    "\n",
    "tauxRéalisation =0.9  # une fermeture à une date, dont report\n",
    "tauxReport=0.1\n",
    "tauxAbsenceIss=0.05 # pour les visites d'une fermeture, taux d'issues sans production d'un rapport\n",
    "lenFer=len(fermetures )\n",
    "if False:  # le process étant aléatoire, le refaire tourner modifie les résultats\n",
    "  fermJours=[]\n",
    "  for k in range(6):\n",
    "    jfTmp=pd.Series([(5+k*8)*7+ int(random()*4)*7+int(random()*4) for i in range(lenFer)],index=fermetures.index,name='jour')\n",
    "    otTmp=pd.Series([456000+k*100+i for i in range(lenFer)],index=fermetures.index,name='OTP')\n",
    "    ordTmp=pd.Series( k,index=fermetures.index,name='ordreVB')\n",
    "    fermJours.append(pd.concat([jfTmp,otTmp,ordTmp],axis=1))\n",
    "  fermJours=pd.concat(fermJours).reset_index()\n",
    "  fermJours['faitFerm']=[choice(a=['OK','RP','KO'],p=[tauxRéalisation- tauxReport,tauxReport,1-tauxRéalisation]) for i in range(lenFer*6)]\n",
    "  fermJours.to_csv('fermJours.csv',index=False)\n",
    "\n",
    "  VisiteIssues=issues.merge(fermJours,on='Fermeture', how='outer')\n",
    "  VisiteIssues['OTF']=VisiteIssues['ordreVB']*300+VisiteIssues['OQ']+456600\n",
    "  VisiteIssues['faitIss']=[random()>tauxAbsenceIss for i in range(len(VisiteIssues))]\n",
    "  VisiteIssues[['Fermeture','ordreVB','faitFerm','faitIss', 'OTF','CodeEx', 'Tatouage', 'jour']].to_csv('VisiteIssues.csv',index=False)\n",
    "  VisiteIssuesFt=VisiteIssues[(VisiteIssues['faitIss'])&(VisiteIssues['faitFerm']!='KO')].copy()\n",
    "  VisiteIssuesFt['minute']=[int(random()*240) for i in range(len(VisiteIssuesFt))]\n",
    "  VisiteIssuesFt['HoroDate']=pd.to_datetime(\"24/\"+VisiteIssuesFt['jour'].astype(str)+\" 23\",format='%y/%j %H')+pd.to_timedelta(VisiteIssuesFt['minute']*60000000000) \n",
    "  VisiteIssuesFt.loc[VisiteIssuesFt['faitFerm']=='RP','HoroDate']=VisiteIssuesFt.loc[VisiteIssuesFt['faitFerm']=='RP','HoroDate']+pd.to_timedelta(21*24*60*60000000000)\n",
    "  VisiteIssuesFt['agent']=[choice(['Karl','Karen', 'Kim','Kamel','Kun']) for k in range(len(VisiteIssuesFt))]\n",
    "  VisiteIssuesFt[[ 'OTF','CodeEx', 'Tatouage','agent','jour', 'HoroDate']].to_csv('VisiteIssuesFt.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du tableau des points de contrôle initial. Ce format devrait changer ...\n",
    "vbIa=pd.read_csv('../_static/controleVB_IA.csv',encoding='UTF-8').iloc[:,1:]\n",
    "lstPC=vbIa['PointControle'].value_counts().reset_index()\n",
    "lstPC['ordrePC']=lstPC['PointControle'].str.split('.').apply(lambda x: x[0]).astype(int).astype(str).str.zfill(2) \n",
    "lstPC=lstPC.sort_values('ordrePC').reset_index(drop=True) #.set_index('ordrePC',drop=True)\n",
    "lstLst=[]\n",
    "for i in range(len(lstPC)) :\n",
    "  vbIi=vbIa[vbIa['PointControle']==lstPC.loc[i,'PointControle']]\n",
    "  lstCom=[]\n",
    "  for j in vbIi.index:\n",
    "    lstCom.append(vbIi.loc[j,'Note'][0]+'_'+vbIi.loc[j,'ResulControle'])\n",
    "  lstLst.append(sorted(lstCom, reverse=False))\n",
    "\n",
    "lstNbrResul=[len(ll) for ll in lstLst]\n",
    "lsP=[[round(.2/(i-1),3)]*(i-1) +[1-(i-1)*round(.2/(i-1),3)]    for i in lstNbrResul]\n",
    "lstLstR=[['R'+str(i) for i in range(j)] for j in lstNbrResul ]\n",
    "\n",
    "tabRC=[]\n",
    "for i in range(len(lstPC)) :\n",
    "  for j in range(lstNbrResul[i]) :\n",
    "    tabRC.append(['P'+ lstPC.loc[i,'ordrePC'], lstPC.loc[i,'PointControle'],lstLstR[i][j],lstLst[i][j],lsP[i][j]  ] )\n",
    "pd.DataFrame(tabRC,columns=['codePC','PC','codeRC', 'RC', 'Probab']).to_csv('controleVB_IS.csv',index=False)\n",
    "# Dans le futur, on devrait entrer directement une table de ce type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création des rapports\n",
    "On lit la table des point&résultats de contrôles et leur probabilité.\n",
    "On lit la liste des visites \"faites\".\n",
    "On génère aléatoirement, pour chaque visite et chaque point de controle, un résultat de contrôle et un commentaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "controleVB_IS=pd.read_csv('controleVB_IS.csv')\n",
    "VisiteIssuesFt=pd.read_csv('VisiteIssuesFt.csv')  # visites d'issues faites\n",
    "VisiteIssuesFt['HoroDate']=pd.to_datetime( VisiteIssuesFt['HoroDate'],format='%Y-%m-%d %H:%M:%S')\n",
    "fermJours=pd.read_csv('fermJours.csv')\n",
    "ComAls=[x for x in  ['Commentaire Aléatoire0 Très compliqué !','Commentaire Aléatoire1 Comment faire ?',\n",
    "    'Commentaire Aléatoire2 Trop long à vous expliquer, on appelera le PCTT demain','Commentaire Aléatoire3 Trois choses à noter',\n",
    "                     'Commentaire Aléatoire4 Sans commentaire']]\n",
    "nbrVBF=len(VisiteIssuesFt)\n",
    "codePCs=controleVB_IS['codePC'].unique()\n",
    "nbrPC=len(codePCs)\n",
    "RCs=[list(controleVB_IS[controleVB_IS['codePC']==codeP]['codeRC']  ) for codeP in codePCs ] # listes des résultats de contrôle par point de contrôle \n",
    "PRBs=[list(controleVB_IS[controleVB_IS['codePC']==codeP]['Probab'] ) for codeP in codePCs ] # listes des probabilité des résultats de contrôle par point de contrôle \n",
    "nbrRCs=[len(x) for x in RCs ]\n",
    "\n",
    "lstRC=[]\n",
    "for k in range(nbrVBF): \n",
    "   for n in range(nbrPC):\n",
    "      lstRC.append([k,codePCs[n],choice(a=RCs[n],p=PRBs[n]),choice(ComAls) ])\n",
    "lstRC=pd.DataFrame(lstRC,columns=['indRap', 'PC','RC','Com'])\n",
    "lstRC.to_csv('lstRC.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=tunnels-dirif, location=US, id=04a2900e-6c5e-4a47-8835-4de0333e8ced>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importation de la table des visites faites dans BigQuery (4 secondes)\n",
    "\n",
    "client  = clientB\n",
    "dataset  = client.dataset('rapports_visites')\n",
    "table = dataset.table('VisiteIssuesFt')\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=[ bigquery.SchemaField(cl, bigquery.enums.SqlTypeNames.STRING) for cl in VisiteIssuesFt.columns],\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    "       autodetect=False,\n",
    "    source_format=bigquery.SourceFormat.CSV\n",
    ")\n",
    "job = client.load_table_from_dataframe( VisiteIssuesFt, table, job_config=job_config)  \n",
    "job.result() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=tunnels-dirif, location=US, id=a16893ca-eb09-4765-9ed4-954e10b89b45>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = dataset.table('lstRC')\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=[ bigquery.SchemaField(cl, bigquery.enums.SqlTypeNames.STRING) for cl in lstRC.columns],\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    "       autodetect=False,\n",
    "    source_format=bigquery.SourceFormat.CSV\n",
    ")\n",
    "job = client.load_table_from_dataframe( lstRC, table, job_config=job_config)  \n",
    "job.result() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les cellules qui suivent correspondent à une ancienne implémentation qui est caduque a vu de cde qui précède.\n",
    "Conservé pour mémoire mais à reprendre entièrement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m lstRC\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nbrVBF): \n\u001b[1;32m----> 5\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnbrPC\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlstRC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mPCs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRCs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPRBs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimuRapp\u001b[39m(ferm,\u001b[38;5;28mord\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def simuRapp(ferm,ord):\n",
    "    vst=VisiteIssues[(VisiteIssues['Fermeture']==ferm)&(VisiteIssues['ordreVB']==ord)].reset_index(drop=True)  \n",
    "    for k in range(len(vst)):\n",
    "        if random()<.95:\n",
    "            dateFichier= str(int((vst.loc[k,'HoroDate']-datetime(2024,1,1)).total_seconds()+1000*random()))\n",
    "            flNm='24-3/ISVB-'+ str(vst.loc[k,'OT'])+'-'+dateFichier+ '.json'\n",
    "            flSr='{\"Tatouage\":\"'+str(VisiteIssues.loc[k,'Tatouage'])+'\",'\n",
    "            flSr=flSr+'\"HoroDate\":\"'+str(VisiteIssues.loc[k,'HoroDate'])+'\",'\n",
    "            flSr=flSr+'\"Agent\":\"'+choice(['Karl','Karen', 'Kim','Kamel','Kun']) +'\"'\n",
    "            for n in range():\n",
    "                cn=controleVB_IS[controleVB_IS['codePC']== controleVB_IS['codePC'].unique()[n] ][['codeRC','Probab']]\n",
    "                flSr=flSr+',\"PC'+str(n)+'\":\"'+choice(a=list(cn['codeRC']),p=list(cn['Probab']))+'\"'+' ,\"CM'+str(n)+'\":\"'+choice(a=ComAls)+'\"'            \n",
    "            flSr=flSr+'}'\n",
    "            with open(flNm,'bw') as fl:\n",
    "                fl.write(flSr.encode('UTF8'))\n",
    "\n",
    "# Création des rapports par application de la fonction avec une probabilité d'omission\n",
    "repertoire ='24-3/'\n",
    "if True :\n",
    "    filenames = next(os.walk(repertoire), (None, None, []))[2]\n",
    "    for fl in filenames:\n",
    "        os.remove(repertoire + fl)\n",
    "\n",
    "    for ferm in list(VisiteIssues['Fermeture'].unique()):\n",
    "        for j in range(6):\n",
    "            if random()<0.5:\n",
    "                simuRapp(ferm,j)\n",
    "filenames = next(os.walk(repertoire), (None, None, []))[2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload des rapports sur le serveur Google Storage\n",
    "On vide le répertoire cible (Storage : issues-secours/rapports-visites/) et  on charge les fichiers avec une pause de 3 secondes \n",
    "pour tenir compte de l'import dans BQ par la fonction qui est déclenchée par le chargement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On vide le répertoire cible (Storage : issues-secours/rapports-visites/) et on recharge les fichiers avec une pause de 3 seconde pour tenir compte de l'import dans BQ\n",
    "bucket = storageC.get_bucket('issues-secours')\n",
    "blobs = storageC.list_blobs(bucket)\n",
    "for blob in blobs:\n",
    "    if 'ISV' in blob.name:\n",
    "        blob.delete()\n",
    "\n",
    "for fl in filenames[:]:\n",
    "    blob = bucket.blob('rapports-visites/'+fl)\n",
    "    blob.upload_from_filename('24-3/'+ fl, if_generation_match= 0)\n",
    "    sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Tatouage', 'HoroDate', 'Agent', 'PC0', 'CM0', 'PC1', 'CM1', 'PC2', 'CM2', 'PC3', 'CM3', 'PC4', 'CM4', 'PC5', 'CM5', 'PC6', 'CM6', 'PC7', 'CM7', 'PC8', 'CM8', 'PC9', 'CM9', 'PC10', 'CM10', 'PC11', 'CM11', 'PC12', 'CM12', 'PC13', 'CM13', 'PC14', 'CM14'])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Liste des clés pour alimenter le schéma d'importation dans BQ\n",
    "name = 'rapports-visites/' +filenames[5]\n",
    "blob = bucket.blob(name)\n",
    "fileContent= (blob.download_as_string(client=None).decode())\n",
    "json.loads(fileContent).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de l'importation dans BQ pour la fonction logDepot\n",
    "client  = bigquery.Client()\n",
    "dataset  = client.dataset('rapports_visites')\n",
    "table = dataset.table('LogDepot')\n",
    "\n",
    "def format_schema(schema):\n",
    "        formatted_schema = []\n",
    "        for row in schema:\n",
    "            formatted_schema.append(bigquery.SchemaField(row,'STRING', 'NULLABLE'))\n",
    "        return formatted_schema\n",
    "lst_schema_VBIS = ['Tatouage', 'HoroDate', 'Agent', 'PC0', 'CM0', 'PC1', 'CM1', 'PC2', 'CM2', 'PC3', 'CM3', 'PC4', 'CM4', 'PC5', 'CM5', 'PC6', 'CM6', \n",
    "                  'PC7', 'CM7', 'PC8', 'CM8', 'PC9', 'CM9', 'PC10', 'CM10', 'PC11', 'CM11', 'PC12', 'CM12', 'PC13', 'CM13', 'PC14', 'CM14']\n",
    "\n",
    "\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.schema = format_schema(lst_schema_VBIS)\n",
    "flJson=json.loads(fileContent)\n",
    "stByt=','.join([flJson[k] for k in lst_schema_VBIS  ]).encode(\"utf-8\")\n",
    "job = client.load_table_from_file(io.BytesIO(stByt), table, job_config = job_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
