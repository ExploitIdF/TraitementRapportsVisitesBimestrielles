{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation des rapports des visites bimestrielles et des interventions de maintenance annuelle des issues et des niches \n",
    "\n",
    "* Définition du calendier et des numéros d'OT (fermeturesJours.csv)\n",
    "* Production des rapports (absences aléatoires)\n",
    "* Upload des rapports sur le serveur\n",
    "\n",
    "Cette simulation permet de tester les choix qui sont faits sur la forme des rapports.  \n",
    "la simulation fournit un jeu de données pour développer les outils de traitements des données pour la visualisation des résultats.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/exploit_diridf/.local/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: plotly in /home/exploit_diridf/.local/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (5.24.1)\n",
      "Requirement already satisfied: google.cloud in /home/exploit_diridf/.local/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (0.34.0)\n",
      "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (3.25.0)\n",
      "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.18.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/exploit_diridf/.local/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/exploit_diridf/.local/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/exploit_diridf/.local/lib/python3.12/site-packages (from plotly->-r requirements.txt (line 3)) (9.0.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly->-r requirements.txt (line 3)) (24.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery->-r requirements.txt (line 5)) (2.19.2)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery->-r requirements.txt (line 5)) (2.34.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery->-r requirements.txt (line 5)) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery->-r requirements.txt (line 5)) (2.7.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery->-r requirements.txt (line 5)) (2.32.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->-r requirements.txt (line 6)) (1.6.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery->-r requirements.txt (line 5)) (1.65.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery->-r requirements.txt (line 5)) (4.25.4)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery->-r requirements.txt (line 5)) (1.24.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery->-r requirements.txt (line 5)) (1.66.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery->-r requirements.txt (line 5)) (1.62.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->-r requirements.txt (line 5)) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->-r requirements.txt (line 5)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->-r requirements.txt (line 5)) (4.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery->-r requirements.txt (line 5)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery->-r requirements.txt (line 5)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery->-r requirements.txt (line 5)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery->-r requirements.txt (line 5)) (2024.8.30)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->-r requirements.txt (line 5)) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "#! pip install -r requirements.txt\n",
    "import pandas as pd, numpy as np\n",
    "import glob, re,json,io,os\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "import plotly.graph_objects as go\n",
    "pd.options.display.max_colwidth = 100\n",
    "from numpy.random import random\n",
    "from numpy.random import choice\n",
    "rng = np.random.default_rng()\n",
    "from google.cloud import bigquery,storage\n",
    "project_id = 'dett-stt'\n",
    "clientB = bigquery.Client(project_id)\n",
    "storageC=storage.Client(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmation des dates de fermetures et simulation des \n",
    "Pour chaque fermeture, on définit aléatoirement 6 dates de visites bimestrielles (champ \"jour\").  \n",
    "Les dates sont identifiées par le rang du jour dans l'année (dayofyear).   \n",
    "On définit les OTs père (champ \"OTP\" : 456000+k*100+i  ). Les OTPs sont dans la plage 456000+456600  \n",
    "On simule que des visites bimestrielles ne sont pas faites ou sont faites, mais reportée.  \n",
    "Pour cela, on définit le champ 'faitFerm' avec mes valeurs (OK : fait à la date programmée, RP: fait mais reporté 21 jours plus tard & KO: non fait).    \n",
    "Paramètres du calcul : ```tauxRéalisation & tauxReport```   \n",
    "\n",
    "On crée la table des visites d'issues programmées `VisiteIssues`\n",
    "On associe aux issues les dates de leurs fermetures programmées .    \n",
    "On définit les OT fils (OTFs) des visites d'issues : ordreVB*300+indexIss+456600  \n",
    "On simule que lors d'une visites bimestrielles certaines issues ne donnent pas lieu à la transmission d'un rapport.   \n",
    "Pour cela, on définit le champ 'faitIss' avec mes valeurs (True / False).  \n",
    "Paramètres du calcul : `tauxAbsenceIss`\n",
    "\n",
    "On crée la table des rapport de visites d'issues transmis (faits) `VisiteIssuesFt` en sélectionnant avec les champs ```faitFerm & faitIss```    \n",
    "On définit l'horodate de transmission du rapport par une valeur aléatoire comprise entre jour + 23h et jour + 27h (jour+1 +3h)   \n",
    "Pour les visites reportées (``` faitIss = 'RP' ```) on calcule une nouvelle horodate 21 jours plus tard (on aurait pu faire varier le report mais on a simplifié)   \n",
    "Enfin, on choisit aléatoirement un nom d'agent (il aurait été plus logique de choisir les agents au niveau de la fermeture mais c'est plus simple comme ça).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "issues=pd.read_csv('https://raw.githubusercontent.com/ExploitIdF/Referentiel_Tunnels/refs/heads/main/issuesFermetures.csv')#[[  'Tatouage',   'Fermeture']]\n",
    "# ['CodeEx', 'PC', 'Tatouage', 'triCode', 'Fermeture']\n",
    "issues['OQ']=range(290)\n",
    "fermetures=issues['Fermeture'].value_counts()\n",
    "\n",
    "tauxRéalisation =0.9  # une fermeture à une date, dont report\n",
    "tauxReport=0.1\n",
    "tauxAbsenceIss=0.05 # pour une visite taux d'issue sans production d'un rapport\n",
    "lenFer=len(fermetures )\n",
    "if False:  # le process étant aléatoire, le refaire tourner modifie les résultats\n",
    "  fermJours=[]\n",
    "  for k in range(6):\n",
    "    jfTmp=pd.Series([(5+k*8)*7+ int(random()*4)*7+int(random()*4) for i in range(lenFer)],index=fermetures.index,name='jour')\n",
    "    otTmp=pd.Series([456000+k*100+i for i in range(lenFer)],index=fermetures.index,name='OTP')\n",
    "    ordTmp=pd.Series( k,index=fermetures.index,name='ordreVB')\n",
    "    fermJours.append(pd.concat([jfTmp,otTmp,ordTmp],axis=1))\n",
    "  fermJours=pd.concat(fermJours).reset_index()\n",
    "  fermJours['faitFerm']=[choice(a=['OK','RP','KO'],p=[tauxRéalisation- tauxReport,tauxReport,1-tauxRéalisation]) for i in range(lenFer*6)]\n",
    "  fermJours.to_csv('fermJours.csv',index=False)\n",
    "\n",
    "  VisiteIssues=issues.merge(fermJours,on='Fermeture', how='outer')\n",
    "  VisiteIssues['OTF']=VisiteIssues['ordreVB']*300+VisiteIssues['OQ']+456600\n",
    "  VisiteIssues['faitIss']=[random()>tauxAbsenceIss for i in range(len(VisiteIssues))]\n",
    "  VisiteIssues[['Fermeture','ordreVB','faitFerm','faitIss', 'OTF','CodeEx', 'Tatouage', 'jour']].to_csv('VisiteIssues.csv',index=False)\n",
    "  VisiteIssuesFt=VisiteIssues[(VisiteIssues['faitIss'])&(VisiteIssues['faitFerm']!='KO')].copy()\n",
    "  VisiteIssuesFt['minute']=[int(random()*240) for i in range(len(VisiteIssuesFt))]\n",
    "  VisiteIssuesFt['HoroDate']=pd.to_datetime(\"24/\"+VisiteIssuesFt['jour'].astype(str)+\" 23\",format='%y/%j %H')+pd.to_timedelta(VisiteIssuesFt['minute']*60000000000) \n",
    "  VisiteIssuesFt.loc[VisiteIssuesFt['faitFerm']=='RP','HoroDate']=VisiteIssuesFt.loc[VisiteIssuesFt['faitFerm']=='RP','HoroDate']+pd.to_timedelta(21*24*60*60000000000)\n",
    "  VisiteIssuesFt['agent']=[choice(['Karl','Karen', 'Kim','Kamel','Kun']) for k in range(len(VisiteIssuesFt))]\n",
    "  VisiteIssuesFt[[ 'OTF','CodeEx', 'Tatouage','agent','jour', 'HoroDate']].to_csv('VisiteIssuesFt.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du tableau des points de contrôle initial. Ce format devrait changer ...\n",
    "vbIa=pd.read_csv('../_static/controleVB_IA.csv',encoding='UTF-8').iloc[:,1:]\n",
    "lstPC=vbIa['PointControle'].value_counts().reset_index()\n",
    "lstPC['ordrePC']=lstPC['PointControle'].str.split('.').apply(lambda x: x[0]).astype(int).astype(str).str.zfill(2) \n",
    "lstPC=lstPC.sort_values('ordrePC').reset_index(drop=True) #.set_index('ordrePC',drop=True)\n",
    "lstLst=[]\n",
    "for i in range(len(lstPC)) :\n",
    "  vbIi=vbIa[vbIa['PointControle']==lstPC.loc[i,'PointControle']]\n",
    "  lstCom=[]\n",
    "  for j in vbIi.index:\n",
    "    lstCom.append(vbIi.loc[j,'Note'][0]+'_'+vbIi.loc[j,'ResulControle'])\n",
    "  lstLst.append(sorted(lstCom, reverse=False))\n",
    "\n",
    "lstNbrResul=[len(ll) for ll in lstLst]\n",
    "lsP=[[round(.2/(i-1),3)]*(i-1) +[1-(i-1)*round(.2/(i-1),3)]    for i in lstNbrResul]\n",
    "lstLstR=[['R'+str(i) for i in range(j)] for j in lstNbrResul ]\n",
    "\n",
    "tabRC=[]\n",
    "for i in range(len(lstPC)) :\n",
    "  for j in range(lstNbrResul[i]) :\n",
    "    tabRC.append(['P'+ lstPC.loc[i,'ordrePC'], lstPC.loc[i,'PointControle'],lstLstR[i][j],lstLst[i][j],lsP[i][j]  ] )\n",
    "pd.DataFrame(tabRC,columns=['codePC','PC','codeRC', 'RC', 'Probab']).to_csv('controleVB_IS.csv',index=False)\n",
    "# Dans le futur, on devrait entrer directement une table de ce type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création des rapports\n",
    "On définit une fonction qui pour une fermeture programmée un jour donné (correspondant à un OT père) enregistre les rapports des issues de la fermeture, avec des omissions aléatoires.  \n",
    "Ensuite, on applique la fonction à toutes les fermetures programmées, avec des omissions aléatoires.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1476"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controleVB_IS=pd.read_csv('controleVB_IS.csv')\n",
    "VisiteIssuesFt=pd.read_csv('VisiteIssuesFt.csv')\n",
    "VisiteIssuesFt['HoroDate']=pd.to_datetime( VisiteIssuesFt['HoroDate'],format='%Y-%m-%d %H:%M:%S')\n",
    "fermJours=pd.read_csv('fermJours.csv')\n",
    "ComAls=[x for x in  ['Commentaire Aléatoire0_Très','Commentaire Aléatoire1_îàèù','Commentaire Aléatoire2_Trop Û','Commentaire Aléatoire3_Trois_Ècî',\n",
    "                     'Commentaire Aléatoire4_Qûàtré']]\n",
    "nbrVBF=len(VisiteIssuesFt)\n",
    "codePCs=controleVB_IS['codePC'].unique()\n",
    "nbrPC=len(codePCs)\n",
    "RCs=[list(controleVB_IS[controleVB_IS['codePC']==codeP]['codeRC']  ) for codeP in codePCs ]\n",
    "PRBs=[list(controleVB_IS[controleVB_IS['codePC']==codeP]['Probab'] ) for codeP in codePCs ]\n",
    "nbrRCs=[len(x) for x in RCs ]\n",
    "\n",
    "nbrVBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstRC=[]\n",
    "for k in range(nbrVBF): \n",
    "   for n in range(nbrPC):\n",
    "      lstRC.append([k,codePCs[n],choice(a=RCs[n],p=PRBs[n]),choice(ComAls) ])\n",
    "lstRC=pd.DataFrame(lstRC,columns=['indRap', 'PC','RC','Com'])\n",
    "lstRC.to_csv('lstRC.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ServiceUnavailable",
     "evalue": "503 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects//jobs?uploadType=multipart: Service Unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidResponse\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/bigquery/client.py:2593\u001b[0m, in \u001b[0;36mClient.load_table_from_file\u001b[0;34m(self, file_obj, destination, rewind, size, num_retries, job_id, job_id_prefix, location, project, job_config, timeout)\u001b[0m\n\u001b[1;32m   2592\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2593\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_multipart_upload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2594\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_resource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\n\u001b[1;32m   2595\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2596\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m resumable_media\u001b[38;5;241m.\u001b[39mInvalidResponse \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/bigquery/client.py:3157\u001b[0m, in \u001b[0;36mClient._do_multipart_upload\u001b[0;34m(self, stream, metadata, size, num_retries, timeout, project)\u001b[0m\n\u001b[1;32m   3153\u001b[0m     upload\u001b[38;5;241m.\u001b[39m_retry_strategy \u001b[38;5;241m=\u001b[39m resumable_media\u001b[38;5;241m.\u001b[39mRetryStrategy(\n\u001b[1;32m   3154\u001b[0m         max_retries\u001b[38;5;241m=\u001b[39mnum_retries\n\u001b[1;32m   3155\u001b[0m     )\n\u001b[0;32m-> 3157\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mupload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransmit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3158\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_http\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_GENERIC_CONTENT_TYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   3159\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/resumable_media/requests/upload.py:153\u001b[0m, in \u001b[0;36mMultipartUpload.transmit\u001b[0;34m(self, transport, data, metadata, content_type, timeout)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_request_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_and_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriable_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_status_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_strategy\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/resumable_media/requests/_request_helpers.py:178\u001b[0m, in \u001b[0;36mwait_and_retry\u001b[0;34m(func, get_status_code, retry_strategy)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m retry_strategy\u001b[38;5;241m.\u001b[39mretry_allowed(total_sleep, num_retries):\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    180\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(wait_time)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/resumable_media/requests/_request_helpers.py:155\u001b[0m, in \u001b[0;36mwait_and_retry\u001b[0;34m(func, get_status_code, retry_strategy)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _CONNECTION_ERROR_CLASSES \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/resumable_media/requests/upload.py:149\u001b[0m, in \u001b[0;36mMultipartUpload.transmit.<locals>.retriable_request\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m result \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    146\u001b[0m     method, url, data\u001b[38;5;241m=\u001b[39mpayload, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    147\u001b[0m )\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/resumable_media/_upload.py:125\u001b[0m, in \u001b[0;36mUploadBase._process_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m \u001b[43m_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_status_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_status_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/resumable_media/_helpers.py:108\u001b[0m, in \u001b[0;36mrequire_status_code\u001b[0;34m(response, status_codes, get_status_code, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m         callback()\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m common\u001b[38;5;241m.\u001b[39mInvalidResponse(\n\u001b[1;32m    109\u001b[0m         response,\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest failed with status code\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    111\u001b[0m         status_code,\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected one of\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;241m*\u001b[39mstatus_codes\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m status_code\n",
      "\u001b[0;31mInvalidResponse\u001b[0m: ('Request failed with status code', 503, 'Expected one of', <HTTPStatus.OK: 200>)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mServiceUnavailable\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      3\u001b[0m table \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mtable(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVisiteIssuesFt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m job_config \u001b[38;5;241m=\u001b[39m bigquery\u001b[38;5;241m.\u001b[39mLoadJobConfig(\n\u001b[1;32m      6\u001b[0m     schema\u001b[38;5;241m=\u001b[39m[ bigquery\u001b[38;5;241m.\u001b[39mSchemaField(cl, bigquery\u001b[38;5;241m.\u001b[39menums\u001b[38;5;241m.\u001b[39mSqlTypeNames\u001b[38;5;241m.\u001b[39mSTRING) \u001b[38;5;28;01mfor\u001b[39;00m cl \u001b[38;5;129;01min\u001b[39;00m VisiteIssuesFt\u001b[38;5;241m.\u001b[39mcolumns],\n\u001b[1;32m      7\u001b[0m     write_disposition\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWRITE_TRUNCATE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m        autodetect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     source_format\u001b[38;5;241m=\u001b[39mbigquery\u001b[38;5;241m.\u001b[39mSourceFormat\u001b[38;5;241m.\u001b[39mCSV\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m job \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_table_from_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mVisiteIssuesFt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_config\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     12\u001b[0m job\u001b[38;5;241m.\u001b[39mresult() \n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/bigquery/client.py:2833\u001b[0m, in \u001b[0;36mClient.load_table_from_dataframe\u001b[0;34m(self, dataframe, destination, num_retries, job_id, job_id_prefix, location, project, job_config, parquet_compression, timeout)\u001b[0m\n\u001b[1;32m   2831\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tmppath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tmpfile:\n\u001b[1;32m   2832\u001b[0m         file_size \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetsize(tmppath)\n\u001b[0;32m-> 2833\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_table_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2834\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtmpfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2835\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2836\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2837\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrewind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2838\u001b[0m \u001b[43m            \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2839\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2840\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjob_id_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_id_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2841\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2842\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2843\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_job_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2844\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2845\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2847\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2848\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(tmppath)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/bigquery/client.py:2597\u001b[0m, in \u001b[0;36mClient.load_table_from_file\u001b[0;34m(self, file_obj, destination, rewind, size, num_retries, job_id, job_id_prefix, location, project, job_config, timeout)\u001b[0m\n\u001b[1;32m   2593\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_multipart_upload(\n\u001b[1;32m   2594\u001b[0m             file_obj, job_resource, size, num_retries, timeout, project\u001b[38;5;241m=\u001b[39mproject\n\u001b[1;32m   2595\u001b[0m         )\n\u001b[1;32m   2596\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m resumable_media\u001b[38;5;241m.\u001b[39mInvalidResponse \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m-> 2597\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_http_response(exc\u001b[38;5;241m.\u001b[39mresponse)\n\u001b[1;32m   2599\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typing\u001b[38;5;241m.\u001b[39mcast(LoadJob, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_from_resource(response\u001b[38;5;241m.\u001b[39mjson()))\n",
      "\u001b[0;31mServiceUnavailable\u001b[0m: 503 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects//jobs?uploadType=multipart: Service Unavailable"
     ]
    }
   ],
   "source": [
    "client  = bigquery.Client()\n",
    "dataset  = client.dataset('rapports_visites')\n",
    "table = dataset.table('VisiteIssuesFt')\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=[ bigquery.SchemaField(cl, bigquery.enums.SqlTypeNames.STRING) for cl in VisiteIssuesFt.columns],\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    "       autodetect=False,\n",
    "    source_format=bigquery.SourceFormat.CSV\n",
    ")\n",
    "job = client.load_table_from_dataframe( VisiteIssuesFt, table, job_config=job_config)  \n",
    "job.result() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=tunnels-dirif, location=US, id=b0e923a1-84b2-4a85-969c-e07ce798f39f>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = dataset.table('lstRC')\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=[ bigquery.SchemaField(cl, bigquery.enums.SqlTypeNames.STRING) for cl in lstRC.columns],\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    "       autodetect=False,\n",
    "    source_format=bigquery.SourceFormat.CSV\n",
    ")\n",
    "job = client.load_table_from_dataframe( lstRC, table, job_config=job_config)  \n",
    "job.result() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m lstRC\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nbrVBF): \n\u001b[1;32m----> 5\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnbrPC\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlstRC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mPCs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRCs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPRBs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimuRapp\u001b[39m(ferm,\u001b[38;5;28mord\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def simuRapp(ferm,ord):\n",
    "    vst=VisiteIssues[(VisiteIssues['Fermeture']==ferm)&(VisiteIssues['ordreVB']==ord)].reset_index(drop=True)  \n",
    "    for k in range(len(vst)):\n",
    "        if random()<.95:\n",
    "            dateFichier= str(int((vst.loc[k,'HoroDate']-datetime(2024,1,1)).total_seconds()+1000*random()))\n",
    "            flNm='24-3/ISVB-'+ str(vst.loc[k,'OT'])+'-'+dateFichier+ '.json'\n",
    "            flSr='{\"Tatouage\":\"'+str(VisiteIssues.loc[k,'Tatouage'])+'\",'\n",
    "            flSr=flSr+'\"HoroDate\":\"'+str(VisiteIssues.loc[k,'HoroDate'])+'\",'\n",
    "            flSr=flSr+'\"Agent\":\"'+choice(['Karl','Karen', 'Kim','Kamel','Kun']) +'\"'\n",
    "            for n in range():\n",
    "                cn=controleVB_IS[controleVB_IS['codePC']== controleVB_IS['codePC'].unique()[n] ][['codeRC','Probab']]\n",
    "                flSr=flSr+',\"PC'+str(n)+'\":\"'+choice(a=list(cn['codeRC']),p=list(cn['Probab']))+'\"'+' ,\"CM'+str(n)+'\":\"'+choice(a=ComAls)+'\"'            \n",
    "            flSr=flSr+'}'\n",
    "            with open(flNm,'bw') as fl:\n",
    "                fl.write(flSr.encode('UTF8'))\n",
    "\n",
    "# Création des rapports par application de la fonction avec une probabilité d'omission\n",
    "repertoire ='24-3/'\n",
    "if True :\n",
    "    filenames = next(os.walk(repertoire), (None, None, []))[2]\n",
    "    for fl in filenames:\n",
    "        os.remove(repertoire + fl)\n",
    "\n",
    "    for ferm in list(VisiteIssues['Fermeture'].unique()):\n",
    "        for j in range(6):\n",
    "            if random()<0.5:\n",
    "                simuRapp(ferm,j)\n",
    "filenames = next(os.walk(repertoire), (None, None, []))[2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload des rapports sur le serveur Google Storage\n",
    "On vide le répertoire cible (Storage : issues-secours/rapports-visites/) et  on charge les fichiers avec une pause de 3 secondes \n",
    "pour tenir compte de l'import dans BQ par la fonction qui est déclenchée par le chargement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On vide le répertoire cible (Storage : issues-secours/rapports-visites/) et on recharge les fichiers avec une pause de 3 seconde pour tenir compte de l'import dans BQ\n",
    "bucket = storageC.get_bucket('issues-secours')\n",
    "blobs = storageC.list_blobs(bucket)\n",
    "for blob in blobs:\n",
    "    if 'ISV' in blob.name:\n",
    "        blob.delete()\n",
    "\n",
    "for fl in filenames[:]:\n",
    "    blob = bucket.blob('rapports-visites/'+fl)\n",
    "    blob.upload_from_filename('24-3/'+ fl, if_generation_match= 0)\n",
    "    sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Tatouage', 'HoroDate', 'Agent', 'PC0', 'CM0', 'PC1', 'CM1', 'PC2', 'CM2', 'PC3', 'CM3', 'PC4', 'CM4', 'PC5', 'CM5', 'PC6', 'CM6', 'PC7', 'CM7', 'PC8', 'CM8', 'PC9', 'CM9', 'PC10', 'CM10', 'PC11', 'CM11', 'PC12', 'CM12', 'PC13', 'CM13', 'PC14', 'CM14'])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Liste des clés pour alimenter le schéma d'importation dans BQ\n",
    "name = 'rapports-visites/' +filenames[5]\n",
    "blob = bucket.blob(name)\n",
    "fileContent= (blob.download_as_string(client=None).decode())\n",
    "json.loads(fileContent).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de l'importation dans BQ pour la fonction logDepot\n",
    "client  = bigquery.Client()\n",
    "dataset  = client.dataset('rapports_visites')\n",
    "table = dataset.table('LogDepot')\n",
    "\n",
    "def format_schema(schema):\n",
    "        formatted_schema = []\n",
    "        for row in schema:\n",
    "            formatted_schema.append(bigquery.SchemaField(row,'STRING', 'NULLABLE'))\n",
    "        return formatted_schema\n",
    "lst_schema_VBIS = ['Tatouage', 'HoroDate', 'Agent', 'PC0', 'CM0', 'PC1', 'CM1', 'PC2', 'CM2', 'PC3', 'CM3', 'PC4', 'CM4', 'PC5', 'CM5', 'PC6', 'CM6', \n",
    "                  'PC7', 'CM7', 'PC8', 'CM8', 'PC9', 'CM9', 'PC10', 'CM10', 'PC11', 'CM11', 'PC12', 'CM12', 'PC13', 'CM13', 'PC14', 'CM14']\n",
    "\n",
    "\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.schema = format_schema(lst_schema_VBIS)\n",
    "flJson=json.loads(fileContent)\n",
    "stByt=','.join([flJson[k] for k in lst_schema_VBIS  ]).encode(\"utf-8\")\n",
    "job = client.load_table_from_file(io.BytesIO(stByt), table, job_config = job_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
