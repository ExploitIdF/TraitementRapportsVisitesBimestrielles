{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation des rapports des visites bimestrielles et des interventions de maintenance annuelle des issues et des niches \n",
    "\n",
    "* Définition du calendier et des numéros d'OT (fermeturesJours.csv)\n",
    "* Production des rapports (absences aléatoires)\n",
    "* Upload des rapports sur le serveur\n",
    "\n",
    "Cette simulation permet de tester les choix qui sont faits sur la forme des rapports.  \n",
    "La simulation fournit un jeu de données pour développer les outils de traitement des données pour la visualisation des résultats.   \n",
    "\n",
    "Les fichiers des issues et des niches et les regroupements en fermetures sont fournis en entrée de ce processus.  \n",
    "Leur élaboration est traitée dans un autre repository : https://github.com/ExploitIdF/Referentiel_Tunnels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -r requirements.txt\n",
    "import pandas as pd, numpy as np\n",
    "import glob, re,json,io,os\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "import plotly.graph_objects as go\n",
    "pd.options.display.max_colwidth = 100\n",
    "from numpy.random import random\n",
    "from numpy.random import choice\n",
    "rng = np.random.default_rng()\n",
    "from google.cloud import bigquery,storage\n",
    "project_id = 'tunnels-dirif'\n",
    "clientB = bigquery.Client(project_id)\n",
    "storageC=storage.Client(project_id)\n",
    "\n",
    "issues=pd.read_csv('https://raw.githubusercontent.com/ExploitIdF/Referentiel_Tunnels/refs/heads/main/issuesFermetures.csv')#[[  'Tatouage',   'Fermeture']]\n",
    "# ['CodeEx', 'PC', 'Tatouage', 'triCode', 'Fermeture']\n",
    "issues['OQ']=range(290)\n",
    "\n",
    "niches=pd.read_csv('https://raw.githubusercontent.com/ExploitIdF/Referentiel_Tunnels/refs/heads/main/niches/nichesFermetures.csv')#[[  'Tatouage',   'Fermeture']]\n",
    "# ['CodeEx', 'Tatouage', 'triCode', 'Sens', 'Fermeture']\n",
    "niches['OQ']=range(len(niches)) # 437\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmation des dates de fermetures\n",
    "Pour chaque fermeture (i: 0-29) , on définit aléatoirement 6 dates ( k:0-5, aussi 'ordreVB') de visites bimestrielles (champ \"jour\").  \n",
    "Les dates sont identifiées par le  *jour de l'année* (dayofyear, 0-365).   \n",
    "On définit les *OTs père* (champ \"OTP\" : 456000+k*100+i  ). Les OTPs sont dans la plage 456000+456600  \n",
    "On simule que des visites bimestrielles ne sont pas faites ou sont faites, mais reportées.  \n",
    "Pour cela, on définit le champ 'faitFerm' avec les valeurs (OK : fait à la date programmée, RP: fait mais reporté 21 jours plus tard & KO: non fait).    \n",
    "Paramètres du calcul : ```tauxRéalisation & tauxReport```   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on reconstruit la liste des fermetures à partir du fichier des issues\n",
    "fermetures=issues['Fermeture'].value_counts()\n",
    "\n",
    "tauxRéalisation =0.9  # Probabilité que les visites d'une fermeture soient réalisées à une date, dont report\n",
    "tauxReport=0.1\n",
    "\n",
    "if False:  # le process étant aléatoire, le refaire tourner modifie les résultats\n",
    "  fermJours=[]\n",
    "  for k in range(6):\n",
    "    jfTmp=pd.Series([(5+k*8)*7+ int(random()*4)*7+int(random()*4) for i in range(lenFer)],index=fermetures.index,name='jour')\n",
    "    otTmp=pd.Series([456000+k*100+i for i in range(lenFer)],index=fermetures.index,name='OTP')\n",
    "    ordTmp=pd.Series( k,index=fermetures.index,name='ordreVB')\n",
    "    fermJours.append(pd.concat([jfTmp,otTmp,ordTmp],axis=1))\n",
    "  fermJours=pd.concat(fermJours).reset_index()\n",
    "  fermJours['faitFerm']=[choice(a=['OK','RP','KO'],p=[tauxRéalisation- tauxReport,tauxReport,1-tauxRéalisation]) for i in range(lenFer*6)]\n",
    "  fermJours.to_csv('fermJours.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visites d'issues et niches\n",
    "On crée la table des visites d'issues programmées `VisiteIssues`\n",
    "On associe aux issues les dates de leurs fermetures programmées .    \n",
    "On définit les OT fils (OTFs) des visites d'issues : ordreVB*800+indexIss+456600  \n",
    "On simule que lors d'une visites bimestrielles certaines issues ne donnent pas lieu à la transmission d'un rapport.   \n",
    "Pour cela, on définit le champ 'faitLocal' avec mes valeurs (True / False).  \n",
    "Paramètres du calcul : `tauxAbsenceIN`\n",
    "\n",
    "On crée la table des rapports de visites d'issues transmis (faits) `VisiteIssuesFt` en sélectionnant avec les champs ```faitFerm & faitLocal```    \n",
    "On définit l'horodate de transmission du rapport par une valeur aléatoire comprise entre jour + 23h et jour + 27h (jour+1 +3h)   \n",
    "Pour les visites reportées (``` faitLocal = 'RP' ```) on calcule une nouvelle horodate 21 jours plus tard (on aurait pu faire varier le report mais on a simplifié)   \n",
    "Enfin, on choisit aléatoirement un nom d'agent (il aurait été plus logique de choisir les agents au niveau de la fermeture mais c'est plus simple comme ça)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fermJours=pd.read_csv('fermJours.csv')  # ['Fermeture', 'jour', 'OTP', 'ordreVB', 'faitFerm']\n",
    "tauxAbsenceIN=0.05 # pour les visites d'une fermeture, taux d'issues sans production d'un rapport\n",
    "lenFer=len(fermJours['Fermeture'].unique() )\n",
    "if True:\n",
    "  VisiteIssues=issues.merge(fermJours,on='Fermeture', how='outer')\n",
    "  VisiteIssues['OTF']=VisiteIssues['ordreVB']*800+VisiteIssues['OQ']+456600\n",
    "  VisiteIssues['faitLocal']=[random()>tauxAbsenceIN for i in range(len(VisiteIssues))]\n",
    "  VisiteIssues['faitLocal']=(VisiteIssues['faitLocal'])&(VisiteIssues['faitFerm']!='KO')\n",
    "  \n",
    "  VisiteIssuesFt=VisiteIssues.copy()\n",
    "  VisiteIssuesFt['minute']=[int(random()*240) for i in range(len(VisiteIssuesFt))]\n",
    "  VisiteIssuesFt['HoroDate']=pd.to_datetime(\"24/\"+VisiteIssuesFt['jour'].astype(str)+\" 23\",format='%y/%j %H')+pd.to_timedelta(VisiteIssuesFt['minute']*60000000000) \n",
    "  VisiteIssuesFt.loc[VisiteIssuesFt['faitFerm']=='RP','HoroDate']=VisiteIssuesFt.loc[VisiteIssuesFt['faitFerm']=='RP','HoroDate']+pd.to_timedelta(21*24*60*60000000000)\n",
    "  VisiteIssuesFt['agent']=[choice(['Karl','Karen', 'Kim','Kamel','Kun']) for k in range(len(VisiteIssuesFt))]\n",
    "  VisiteIssuesFt=VisiteIssuesFt[[ 'OTF','CodeEx', 'Tatouage','agent','jour', 'HoroDate','Fermeture','ordreVB','faitFerm','faitLocal']].sort_values('OTF')\n",
    "  VisiteIssuesFt.to_csv('VisiteIssuesFt.csv',index=False)\n",
    "\n",
    "  VisiteNiches=niches.merge(fermJours,on='Fermeture', how='outer')\n",
    "  VisiteNiches['OTF']=VisiteNiches['ordreVB']*800+VisiteNiches['OQ']+456600 + 300\n",
    "  VisiteNiches['faitLocal']=[random()>tauxAbsenceIN for i in range(len(VisiteNiches))]\n",
    "  VisiteNiches['faitLocal']=(VisiteNiches['faitLocal'])&(VisiteNiches['faitFerm']!='KO')\n",
    "\n",
    "  VisiteNichesFt=VisiteNiches.copy()\n",
    "  VisiteNichesFt['minute']=[int(random()*240) for i in range(len(VisiteNichesFt))]\n",
    "  VisiteNichesFt['HoroDate']=pd.to_datetime(\"24/\"+VisiteNichesFt['jour'].astype(str)+\" 23\",format='%y/%j %H')+pd.to_timedelta(VisiteNichesFt['minute']*60000000000) \n",
    "  VisiteNichesFt.loc[VisiteNichesFt['faitFerm']=='RP','HoroDate']=VisiteNichesFt.loc[VisiteNichesFt['faitFerm']=='RP','HoroDate']+pd.to_timedelta(21*24*60*60000000000)\n",
    "  VisiteNichesFt['agent']=[choice(['Karl','Karen', 'Kim','Kamel','Kun']) for k in range(len(VisiteNichesFt))]\n",
    "  VisiteNichesFt[[ 'OTF','CodeEx', 'Tatouage','agent','jour', 'HoroDate','Fermeture','ordreVB','faitFerm','faitLocal']].to_csv('VisiteNichesFt.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création des rapports\n",
    "On définit une fonction qui a pour variable un nombre de visites et une liste de point de contrôle et qui génère les valeurs des résultats de contrôle pour chaque visite.     \n",
    "Pour cela, on génère aléatoirement, pour chaque visite et chaque point de controle, un résultat de contrôle et un commentaire.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcRc=pd.read_csv('https://raw.githubusercontent.com/ExploitIdF/TraitementRapportsVisitesBimestrielles/refs/heads/master/controlesIsNi.csv')\n",
    "ComAls=[x for x in  ['Commentaire Aléatoire0 Très compliqué !','Commentaire Aléatoire1 Comment faire ?',\n",
    "    'Commentaire Aléatoire2 Trop long à vous expliquer, on appelera le PCTT demain','Commentaire Aléatoire3 Trois choses à noter',\n",
    "                     'Commentaire Aléatoire4 Sans commentaire']]\n",
    "def simRC(lsPc,nbrV):\n",
    "   pcRcS=pcRc[pcRc['codePC'].str[1:].astype(int).isin(lsPc)]\n",
    "   pcS=list(pcRcS['codePC'].unique())\n",
    "   nbrPC=len(pcS)\n",
    "   RCs=[list(pcRcS[pcRcS['codePC']==codeP]['codeRC']  ) for codeP in pcS ] # listes des résultats de contrôle par point de contrôle \n",
    "   PRBs=[list(pcRcS[pcRcS['codePC']==codeP]['Probab'] ) for codeP in pcS ] # listes des probabilité des résultats de contrôle par point de contrôle \n",
    "   nbrRCs=[len(x) for x in RCs ]\n",
    "   lstRC=[]\n",
    "   for k in range(nbrV): \n",
    "      for n in range(nbrPC):\n",
    "         lstRC.append([k,pcS[n],choice(a=RCs[n],p=PRBs[n]),choice(ComAls) ])\n",
    "   return pd.DataFrame(lstRC,columns=['indRap', 'PC','RC','Com'])\n",
    "\n",
    "VisiteIssuesFt=pd.read_csv('VisiteIssuesFt.csv')  # visites d'issues faites\n",
    "nbrV=len(VisiteIssuesFt)\n",
    "simRC(range(9), nbrV).to_csv('lstRCNi.csv',index=False)\n",
    "\n",
    "nbrV=len(pd.read_csv('VisiteIssuesFt.csv') )\n",
    "simRC(range(0,22), nbrV).to_csv('lstRCIs.csv',index=False)\n",
    "\n",
    "nbrV=len(pd.read_csv('VisiteNichesFt.csv') )\n",
    "simRC(range(22,26), nbrV).to_csv('lstRCNi.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importTab(nomTab):\n",
    "    tab=pd.read_csv(nomTab+ '.csv')\n",
    "    dataset  = clientB.dataset('rapports_visites')\n",
    "    table = dataset.table(nomTab)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=[ bigquery.SchemaField(cl, bigquery.enums.SqlTypeNames.STRING) for cl in tab.columns],\n",
    "        write_disposition=\"WRITE_TRUNCATE\",\n",
    "        autodetect=False,\n",
    "        source_format=bigquery.SourceFormat.CSV\n",
    "    )\n",
    "    job = clientB.load_table_from_dataframe( tab, table, job_config=job_config)  \n",
    "    job.result() \n",
    "\n",
    "for nomTab in ['VisiteIssuesFt','VisiteNichesFt','lstRCNi','lstRCIs' ]:\n",
    "    importTab(nomTab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload des rapports sur le serveur Google Storage\n",
    "On vide le répertoire cible (Storage : issues-secours/rapports-visites/) et  on charge les fichiers avec une pause de 3 secondes \n",
    "pour tenir compte de l'import dans BQ par la fonction qui est déclenchée par le chargement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On vide le répertoire cible (Storage : issues-secours/rapports-visites/) et on recharge les fichiers avec une pause de 3 seconde pour tenir compte de l'import dans BQ\n",
    "bucket = storageC.get_bucket('issues-secours')\n",
    "blobs = storageC.list_blobs(bucket)\n",
    "for blob in blobs:\n",
    "    if 'ISV' in blob.name:\n",
    "        blob.delete()\n",
    "\n",
    "for fl in filenames[:]:\n",
    "    blob = bucket.blob('rapports-visites/'+fl)\n",
    "    blob.upload_from_filename('24-3/'+ fl, if_generation_match= 0)\n",
    "    sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Tatouage', 'HoroDate', 'Agent', 'PC0', 'CM0', 'PC1', 'CM1', 'PC2', 'CM2', 'PC3', 'CM3', 'PC4', 'CM4', 'PC5', 'CM5', 'PC6', 'CM6', 'PC7', 'CM7', 'PC8', 'CM8', 'PC9', 'CM9', 'PC10', 'CM10', 'PC11', 'CM11', 'PC12', 'CM12', 'PC13', 'CM13', 'PC14', 'CM14'])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Liste des clés pour alimenter le schéma d'importation dans BQ\n",
    "name = 'rapports-visites/' +filenames[5]\n",
    "blob = bucket.blob(name)\n",
    "fileContent= (blob.download_as_string(client=None).decode())\n",
    "json.loads(fileContent).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de l'importation dans BQ pour la fonction logDepot\n",
    "client  = bigquery.Client()\n",
    "dataset  = client.dataset('rapports_visites')\n",
    "table = dataset.table('LogDepot')\n",
    "\n",
    "def format_schema(schema):\n",
    "        formatted_schema = []\n",
    "        for row in schema:\n",
    "            formatted_schema.append(bigquery.SchemaField(row,'STRING', 'NULLABLE'))\n",
    "        return formatted_schema\n",
    "lst_schema_VBIS = ['Tatouage', 'HoroDate', 'Agent', 'PC0', 'CM0', 'PC1', 'CM1', 'PC2', 'CM2', 'PC3', 'CM3', 'PC4', 'CM4', 'PC5', 'CM5', 'PC6', 'CM6', \n",
    "                  'PC7', 'CM7', 'PC8', 'CM8', 'PC9', 'CM9', 'PC10', 'CM10', 'PC11', 'CM11', 'PC12', 'CM12', 'PC13', 'CM13', 'PC14', 'CM14']\n",
    "\n",
    "\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.schema = format_schema(lst_schema_VBIS)\n",
    "flJson=json.loads(fileContent)\n",
    "stByt=','.join([flJson[k] for k in lst_schema_VBIS  ]).encode(\"utf-8\")\n",
    "job = client.load_table_from_file(io.BytesIO(stByt), table, job_config = job_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
